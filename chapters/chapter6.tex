\chapter{Porting Xen Network Virtualization\label{cha:chapter6}}
In this chapter, Xen network virtualization architecture will be explained in the first part and then the work related to porting it to PHIDIAS will be presented in second part. Other split drivers for different I/O devices can ported using the same approach adopted for PV network drivers with zero or little changes if required.

\section{Xen Network Virtualization \label{sec:xennetwork}}
This section will give an overview of Xen network's virtualization architecture and data flow. Details of bringing up virtual network interfaces in Xen will also be explained. Figure \ref{xennet_arch} shows basic architecture of Xen network virtualization framework.
\begin{figure}[!htbp]
	\centering
	\includegraphics[width=10cm]{xennet_arch}
	\caption{Xen network virtualization architecture}
	\label{xennet_arch}
\end{figure}

\subsection{Backend and Frontend drivers of XenBus \label{sec:xenbus}}
All Xen split I/O drivers depend upon a general bus entity named XenBus which provides an interface to backend and frontend drivers for establishing communication between each other. The internal working of Xenbus is dependent upon Xenstore and event channels. Xenbus, itself, is composed of two split drivers that work together to provide functionality of inter-domain communication as shown in Figure \ref{xenbus_xs}. These two halves of XenBus split driver model are:
\begin{itemize}
	\item \textbf{XenBus Backend} is a bus that itself is registered with kernel bus subsystem. It is responsible for enumerating all backend devices in Xenstore, calling their corresponding probe functions and watching xenstore for changes. All backend drivers register themselves with XenBus backend.
	\item \textbf{XenBus Frontend} is also a type of bus which registers itself with kernel bus subsystem. It is responsible for enumerating and probing all frontend devices and registering their watch points in Xenstore for getting notification about changes in the backend devices. All frontend drivers register themselves with XenBus frontend.
\end{itemize}
\begin{figure}[!htbp]
	\centering
	\includegraphics[width=10cm]{xenbus_xs}
	\caption{XenBus Architecture for connecting backends with frontends in Xen}
	\label{xenbus_xs}
\end{figure}

\subsection{Netback Driver \label{sec:netback}}
Netback driver implements the backend in Dom0. It communicates with network frontend through two rings i.e. transmit Tx ring and receive Rx ring. These rings are shared by network frontend driver and netback maps them into Dom0 address space. Netback driver is responsible for sending/receiving network packets to/from actual network hardware using native NIC driver. 

\subsection{Netfront Driver \label{sec:netfront}}
Netfront driver implements the frontend in unprivileged domains. It creates Tx and Rx rings for sending instructions about data flow to the network backend driver. These rings do not contain actual data packets. The data packets are transferred using shared memory pages offered by grant table mechanism of Xen. Both frontend and backend notifies each other of requests and responses through event channel.

\subsection{Initialization of Network Split Drivers in Xen \label{sec:initnet}}
\subsubsection{Probing of Netback Driver}
In native Xen setup, netback driver is probed when Xend toolstack in Dom0 creates \textbf{`vif'} device. In the netback\_probe function, network backend sets its state to XenbusStateInitialising. It then write keys to the Xenstore related to network features it supports. It includes feature-sg, feature-gso-tcpv4, feature-gso-tcpv6,feature-ipv6-csum-offload, feature-rx-copy,feature-rx-flip, feature-multicast-control, feature-dynamic-multicast-control,feature-split-event-channels, multi-queue-max-queues and feature-ctrl-ring. It then reads the main script provided in DomU's XL configuration file for interfacing virtual network interface with real network device and kickoffs other necessary scripts to setup the bridging between virtual and real network interface. There are three main types of network setup supported by Xen i.e. bridged, routed and NAT \cite{xen_network}. Its default mode is bridged and from Xen 4.3 onwards, support of openvswitch is also provided.
\subsubsection{Probing of Netfront Driver}
Netfront driver gets probed by xenbus\_probe function during its initialization only if Xenstore and netback are up and running. Netfront probe function calls xennet\_create\_dev to create a net\_device and registers this device with Linux Network stack. It also allocates the transmit queue and receive queues for this net\_device. After DomU is started and netfront driver is initialized successfully, hotplug scripts in Dom0 sets the initial state of netfront to XenbusStateInitialising by writing corresponding netfront state key in Xenstore. Both netback and netfront has registered a notifier with Xenstore through Xenbus which watches the state of other end and get notifications from Xenstore upon changes in state of other end. Setting of the state of netfront to XenbusStateInitialising causes netback to be notified for changing its bus state to XenbusStateInitWait. When netfront driver becomes notified about XenbusStateInitWait state of netback, it calls xennet\_connect function. xennet\_connect is the main function which does all the necessary work to connect netfront with netback. It  creates Tx and Rx shared rings, grants access permissions to netback, allocates Tx and Rx event channels and write tx-ring-ref, rx-ring-ref, event-channel-tx, event-channel-rx request-rx-copy, feature-rx-notify, feature-sg and feature-gso-tcpv4 to Xenstore filesystem. It also allocates Rx buffers for receiving packets from netback. After completing its talk with netback, it sets its state to XenbusStateConnected. 
\subsubsection{Connection of Netfront and Netback}
On receiving state change notification about successful connection from netfront, netback reads the keys written by netfront in Xenstore filesystem. It reads grant references of Tx and Rx rings and maps them into its address space. It binds its event channels to netfront tx and rx event channels. It then finally sets its state to XenbusStateConnected.


\subsection{Network Data Flow from Netfront to Netback \label{sec:tx}}
For transmitting network packets to netback, netfront calls xennet\_start\_xmit function. It examines received skb\_buff from network stack which is basic structure used by Linux kernel to manage network packets. First, it allocates grant reference and corresponding Tx request for linear part of skb\_buff. Then it calculates the number of fragments of network packet and allocates grant references and Tx requests for remaining fragmented portion of skb\_buff. Finally, it notifies netback driver of Tx requests in Tx ring. 
\\
\\
On getting notification from netfront about Tx requests, netback processes the requests in xenvif\_tx\_action and creates corresponding copy and map operations to be performed on shared grant references. Netback driver has the maximum length of 128 bytes for TX copy operation. If the packet size is larger than 128 bytes, remaining data is mapped into Dom0 address space. After filling out a newly allocated skb\_buff with received network packet contents, netback gives this packet to Linux network stack which forwards it to upper layers.

\subsection{Network Data Flow from Netback to Netfront \label{sec:rx}}
For transmission of packets from netback to netfront, xenvif\_start\_xmit is called. It first checks the number of available of Rx buffer slots in RX ring of netfront, mapped into Dom0's address space. After getting required number of slots in RX ring, it copies the packet data into those mapped Rx buffers through gnttab\_batch\_copy function. This in turn calls hypercall responsible for transferring data into netfront Rx buffers. Netback driver relies heavily on this batch copy operation for copying data to and from an unprivileged guest via the grant references in the RX and TX ring buffers. Netfront driver, on the other end, calls xennet\_poll function to retrieve the sent network packets and forwards it to Linux network stack.

\section{Architecture of Ported Network Virtualization Framework to PHIDIAS\label{sec:xennetphidias}}
In this section, changes required for porting Xen network split drivers to PHIDIAS will be illustrated. 

\subsection{Writing Keys to Xenstore for Network Virtualization \label{sec:keys}}
For successful initialization and inter-domain communication of I/O split drivers, Xenstore should be up and running. In case of our setup, Xenstore was cross-compiled using build root method as described in Xen wiki page \cite{cross_compile}. It was started in Dom0 Linux guest using the commands in listing \ref{commands}.

\begin{lstlisting}[caption= Commands for startin Xenstore dameon in Dom0 on PHIDIAS , label={commands},]
#mount -t xenfs xenfs /proc/xen
#cp -r /usr/local/lib/* /lib
#mkdir /var/run
#touch /var/run/xenstored.pid
#touch /var/lib/xenstored/tdb
#chmod +x /usr/local/sbin/xenstored
#./usr/local/sbin/xenstored --pid-file /var/run/xenstored.pid --priv-domid 1

\end{lstlisting}

First command mounts the Xen filesystem to provide access to the shared Xenstore pages of Dom0 and DomU to user-space. It also creates a file for accessing event channel number of Dom0 to bind with Xenstore's event channel. The last command basically starts the Xenstore in daemon mode and gives access to DomU of ID 1 to write or read keys in Xenstore filesystem. By default, only Dom0 has permissions to write or modify keys in Xenstore. It can grant permission to other guests by using \textbf{xenstore-chmod} command but it would require Dom0 to know beforehand all the keys information which DomU would need to read/write/modify in Xenstore. The simpler approach is to start Xenstore daemon with \textbf{priv-domid 1} which will enable guest with Domain ID 1 to read/write/modify all keys in Xenstore.

\subsection{Probing Netback driver \label{sec:probenetback}}
As described in section \ref{sec:initnet}, netback driver is probed by Xend service of Xen toolstack .In case of our setup, since there was no Xend service, netback driver gets probed by Xenstore dameon by notifying the Dom0 guest after being started successfully. No modification was done in the code as Xen already supports probing netback by Xenstore. However, there are some keys which were written by Xend and Xendomains services in Dom0 on native Xen setup. These were manually written to Xenstore by Dom0 in our ported setup, by using the script in listing \ref{script_startup}.

\begin{lstlisting}[caption= Script used in Dom0 to write necessary keys for network drivers to Xenstore on PHIDIAS ,label={script_startup}]
#!/bin/bash

cd /usr/local/bin/
chmod +x *

./xenstore-write /local/domain/0/domid 0
./xenstore-write /local/domain/0/name Domain-0
./xenstore-write /local/domain/0/control/shutdown ""
./xenstore-write /local/domain/0/control/feature-poweroff 0
./xenstore-write /local/domain/0/control/feature-halt 0
./xenstore-write /local/domain/0/control/feature-suspend 0
./xenstore-write /local/domain/0/control/feature-reboot 0
./xenstore-write /local/domain/1/device ""
./xenstore-write /local/domain/1/device/vif ""
./xenstore-write /local/domain/1/device/vif/0 ""
./xenstore-write /local/domain/1/device/vif/0/backend-id 0
./xenstore-write /local/domain/1/device/vif/0/mac "D2:A3:CD:27:4A:53"
./xenstore-write /local/domain/1/device/vif/0/backend "/local/domain/0/backend/vif/1/0"
./xenstore-write /local/domain/0/backend/vif/1/0/frontend-id 1
./xenstore-write /local/domain/0/backend/vif/1/0/frontend "/local/domain/1/device/vif/0"
./xenstore-write /local/domain/0/backend/vif/1/0/script "/etc/xen/scripts/vif-route"
./xenstore-write /local/domain/0/backend/vif/1/0/handle 0
./xenstore-write /local/domain/0/backend/vif/1/0/mac "D2:A3:CD:27:4A:53"
./xenstore-write /local/domain/1/device/vif/0/state 1
./xenstore-write /local/domain/0/backend/vif/1/0/state 1

\end{lstlisting}

\subsection{Probing Netfront driver \label{sec:probenetfront}}
As described in section \ref{sec:initnet}, netfront driver gets probed when XL toolstack in Dom0 creates and starts DomU. In PHIDIAS, both Dom0 and DomU boots at the same time. Netfront driver should only probed after running Xenstore and netback driver in Dom0. To notify DomU guest about running Xenstore daemon in Dom0 and probing netfront driver, an interprocess interrupt was used. An IPC capability with index 2 was added in configuration options of Dom0 linux guest in PHIDIAS as shown in listing \ref{ipc_cap}.

\begin{lstlisting}[caption= Capability added in config options of Dom0 linux guest to notify DomU guest to probe netfront driver ,label={ipc_cap}]
    <cap type="ipc" target_xref="linux2" param="0xa" /> 
\end{lstlisting}

SGI number 10 was used for this capability. In netfront driver, an IPI handler, listed in \ref{ipi_handler}, was registered which was called on receiving an IPI interrupt from Dom0.

\begin{lstlisting}[caption= IPI handler for receiving notification from Dom0 about successful running of Xenstore and netback driver ,label={ipi_handler}]
static irqreturn_t irqHandlerBusProbe (int irq, void *dev_id)
{
   schedule_work(&probe_work);
   return IRQ_HANDLED;
}

\end{lstlisting}
To trigger an IPI from Dom0 from DomU, a small kernel module was implemented which triggered the capability with index 2. This module was inserted after running Xenstore daemon and writing network related Xenstore keys by Dom0 guest. Code for triggering this capability is shown in slisting \ref{trigger}.

\begin{lstlisting}[caption=Code to trigger capability 2 from Dom0 to DomU ,label={trigger}]
       unsigned int val = 2;
       asm volatile("mov x0, #0x9999\n\tmov x1, %0\n\thvc #0" :: "r" (val) : "x0", "x1");
\end{lstlisting}

\subsection{Transmission of packets from netfront to netback \label{sec:txnetfront}}
For transmitting packets from netfront to netback, grant entries are created in Tx ring by netfront which contain page frame numbers of shared pages containing network packet data. As described in section \ref{sec:splitdriverusage}, a ZONE\_XEN was created for sharing memory pages between frontend and backend drivers. Since Linux network stack allocates skb\_buff from Linux \textbf{NORMAL} memory zone in native Xen netfront driver, it should be copied into ZONE\_XEN to share it netback driver. For this, a function xen\_skb\_copy was implemented in skbuff.c file to copy skb\_buff from NORMAL memory zone to XEN zone  and called in xennet\_start\_xmit. Linux network stack provides a function skb\_copy for copying both an sk\_buff head and its data from non-linear to linear buffers. This had been used as a reference for our implementation of xen\_skb\_copy which is provided in listing \ref{xen_skb_copy}.
\\
\\
\begin{lstlisting}[caption=Code snippte of function xen\_skb\_copy for copying entire network packet from NORMAL memory zone to XEN memory zone and its invocation from xennet\_start\_xmit from ,label={xen_skb_copy},frame=single,style=base]
`File : net/core/skbuff.c `

struct sk_buff *xen_skb_copy(const struct sk_buff *skb, gfp_t gfp_mask)
{
    int headerlen = skb_headroom(skb);
    unsigned int size = skb_end_offset(skb) + skb->data_len;
    struct sk_buff *n = __xen_alloc_skb(size, gfp_mask);
    if (!n)
        return NULL;

    /* Set the data pointer */
    skb_reserve(n, headerlen);
    /* Set the tail pointer and length */
    skb_put(n, skb->len);

    if (skb_copy_bits( skb, -headerlen, n->head, headerlen + skb->len))
        BUG();

    copy_skb_header(n, skb);
    return n;
}
 ..
` File : drivers/net/xen-netfront.c `
 
 static int xennet_start_xmit(struct sk_buff *skb, struct net_device *dev)
 {
 ...
     struct sk_buff *nskb;
  @   struct sk_buff *xen_skb;@

    /* Drop the packet if no queues are set up */
    if (num_queues < 1)
        goto drop;
        
  @  xen_skb = xen_skb_copy(skb, GFP_XEN);
    if (!xen_skb)
        goto drop;
 
    dev_kfree_skb_any(skb);
    skb = xen_skb;@

    queue_index = skb_get_queue_mapping(skb);
    queue = &np->queues[queue_index];
...
}

\end{lstlisting}

The above copy operation resulted in degradation of the networking performance for large packet sizes. The resulting affects will shown for throughput testing in later chapter \ref{cha:chapter7}.

\subsection{Receiving packets from netfront by netback \label{sec:rxnetfront}}
In native Xen setup, netback driver uses GNTTABOP\_map\_grant\_ref hypercall for mapping shared network packets into its domain. This hypercall operation adds a mapping to the provided host virtual address in Dom0. Actually, Dom0 allocates pages from NORMAL memory zone in its address space and provides virtual addresses of those pages as host addresses in  GNTTABOP\_map\_grant\_ref hypercall. Xen hypervisor then creates a mapping of shared pages to point to these host virtual addresses in Dom0's page tables. Since netback allocates pages for virtual host addresses from its own NORMAL memory zone, it has corresponding struct page definitions for all those pages in its mem\_map array \cite{mem_map} and can safely use virt\_to\_page macro while filling fragments of skb\_buff for received packets. In Linux, network stack uses this struct page definitions in functions related to dealing with fragments of network packets. The macro virt\_to\_page returns struct page's of given virtual address which is then used in function xenvif\_fill\_frags to fill skb\_buff fragments via \_skb\_fill\_page\_desc function.
\\
\\
In our ported setup, shared pages are mapped using ioremap. The macro virt\_to\_page cannot be used for virtual addresses returned by ioremap. Dom0 only allocates struct page's for the memory range given to it by PHIDIAS. It has no struct page's for DomU's shared memory, otherwise its kernel's buddy allocator  \cite{buddy} can allocate pages from DomU's memory. In order to solve this problem, netback driver in PHIDIAS maps the shared pages containing network data and then copies data into its newly allocated pages from NORMAL memory zone as shown in Figure \ref{copy}. It then forwards these pages to Linux network stack for further processing. Since large fragmented packets span multiple pages, this additional copy operation in our implementation had resulted in decreased network performance which will be shown later \ref{cha:chapter7}.
\begin{figure}[!htbp]
\begin{center}
\begin{tikzpicture}

%\draw[step=0.5cm, gray, very thin] (0,-8) grid (10,3);
\node[text width=6cm, anchor=west, right] at (4,5)
{ \textbf{\huge{Privileged Linux Guest/Dom0}}};
\node[text width=6cm, anchor=west, right] at (12,5)
{ \textbf{\huge{Unprivileged Linux Guest/DomU}}};
%\node at (0,1.5) [rectangle, draw=black, thick, rounded corners, fill=black!5, minimum height = 1cm, minimum width = 4.5cm, anchor=south west] (nts) {Network Stack};

\node at (0,0) [rectangle, draw=black, thick, rounded corners, fill=yellow!20, minimum height = 2.9cm, minimum width = 5cm, anchor=south west] (ntw) {};
\node[below right, inner sep=4pt, text width=5cm] at (ntw.north west) { \footnotesize{Pages allocated by Netback for network buffers (SKB buffers) from kernel's NORAML zone}};
\node at (0.25,0.25) [rectangle, draw=black, thick, rounded corners, fill=black!5, minimum height = 0.5cm, minimum width = 1cm, anchor=south west] (nva1) {va};
\node at (1.5,0.25) [rectangle, draw=black, thick, rounded corners, fill=black!5, minimum height = 0.5cm, minimum width = 1cm, anchor=south west] (nva2) {va};
\node at (3.0,0.25) [rectangle, draw=black, thick, rounded corners, fill=black!5, minimum height = 0.5cm, minimum width = 1cm, anchor=south west] (nva3) {va};

\node at (6,0) [rectangle, draw=black, thick, rounded corners, fill=yellow!20, minimum height = 2.9cm, minimum width = 5cm, anchor=south west] (ntw2) {};
\node[below right, inner sep=4pt, text width=5cm] at (ntw2.north west) { \footnotesize{Pages mapped by netback using ioremap into Domain-0 address space}};
\node at (6.25,0.25) [rectangle, draw=black, thick, rounded corners, fill=black!5, minimum height = 0.5cm, minimum width = 1cm, anchor=south west] (nva11) {va};
\node at (7.5,0.25) [rectangle, draw=black, thick, rounded corners, fill=black!5, minimum height = 0.5cm, minimum width = 1cm, anchor=south west] (nva12) {va};
\node at (9.0,0.25) [rectangle, draw=black, thick, rounded corners, fill=black!5, minimum height = 0.5cm, minimum width = 1cm, anchor=south west] (nva13) {va};


\node at (12,0) [rectangle, draw=black, thick, rounded corners, fill=red!10, minimum height = 2.9cm, minimum width = 5cm, anchor=south west] (skb) {};
\node[below right, inner sep=4pt,text width=4.9cm] at (skb.north west) {\scriptsize{Pages allocated by Netfront for TX SKB buffers from ZONE XEN to be shared with Netback}};

\node at (12.25,0.25) [rectangle, draw=black, thick, rounded corners, fill=black!5, minimum height = 0.5cm, minimum width = 1cm, anchor=south west] (sva1) {va};
\node at (13.5,0.25) [rectangle, draw=black, thick, rounded corners, fill=black!5, minimum height = 0.5cm, minimum width = 1cm, anchor=south west] (sva2) {va};
\node at (15,0.25) [rectangle, draw=black, thick, rounded corners, fill=black!5, minimum height = 0.5cm, minimum width = 1cm, anchor=south west] (sva3) {va};

\node at (2.5,-3.5) [rectangle, draw=black, thick, fill=yellow!20, minimum height = 2cm, minimum width = 1.5cm, anchor=south west] (npg1) {};
\node at (3.5,-3) [rectangle, draw=black, thick, fill=yellow!20, minimum height = 2cm, minimum width = 1.5cm, anchor=south west] (npg2) {};
\node at (4.5,-2.5) [rectangle, draw=black, thick, fill=yellow!20, minimum height = 2cm, minimum width = 1.5cm, anchor=south west] (npg3) {};
\node[inner sep=0pt, text width=1cm] at (npg3.center) {\scriptsize{Local Page}};

\node at (11,-3.5) [rectangle, draw=black, thick, fill=red!10, minimum height = 2cm, minimum width = 1.5cm, anchor=south west] (spg1) {};
\node at (10,-3) [rectangle, draw=black, thick, fill=red!10, minimum height = 2cm, minimum width = 1.5cm, anchor=south west] (spg2) {};
\node at (9,-2.5) [rectangle, draw=black, thick, fill=red!10, minimum height = 2cm, minimum width = 1.5cm, anchor=south west] (spg3) {};
\node[inner sep=0pt, text width=1cm] at (spg3.center) {\scriptsize{Shared Page}};


\begin{scope}[>=latex]
	\draw [thick, dotted, ->] (nva1.east) -- (nva2.west);
	\draw [thick, dotted, ->] (nva2.east) -- (nva3.west);
	\draw [thick, ->] (nva1.south) |- (npg1.north west);
	\draw [thick, ->] (nva2.south) |- (npg2.north west);
	\draw [thick, ->] (nva3.south) |- (npg3.north west);

	\draw [thick, ->] (nva13) -- (spg3);
	\draw [thick, ->] (nva12.south) |- (spg2.south west);
	\draw [thick, ->] (nva11.south) |- (spg1.south west);
	
	\draw [thick, dotted, ->] (sva1.east) -- (sva2.west);
	\draw [thick, dotted, ->] (sva2.east) -- (sva3.west);
	\draw [thick, ->] (sva1.south) |- (spg3.north east);
	\draw [thick, ->] (sva2.south) |- (spg2.north east);
	\draw [thick, ->] (sva3.south) |- (spg1.north east);
	\draw [dashed] (11.5,6) -- (11.5,-6);
	\draw [very thick, dashed, ->,postaction={decorate,decoration={raise=-3ex,text along path,reverse path,text align=center,text={|\sffamily|copying network buffer contents from shared to local pages}}}]  (spg1.south) to [bend left=65] (npg1.south);
	\draw [very thick, dashed, ->] (spg2.south) to [bend left=65] (npg2.south);
	\draw [very thick, dashed, ->] (spg3.south) to [bend left=65] (npg3.south);
\end{scope}

\end{tikzpicture}
\end{center}
\caption{Netback copy operation in TX path from netfront to netback on PHIDIAS}
\label{copy}
\end{figure}



\section{Porting other virtual I/O devices in Xen \label{sec:otherio}}
In this section, steps require for porting remaining Xen I/O virtual devices, besides network, will be discussed.  One of the core virtual devices in Xen is block device which provides a non-volatile storage to guests to store and retain data between power reboots.

\subsection{Xen Virtual Block Device Driver \label{sec:block}}
Xen virtual block device driver provides an interface to an abstract block device, typically a virtual hard disk backed by real disks, individual partitions, or even files on a host filesystem \cite{xen_book}. Just like other split drivers, it consists of block frontend and backend drivers and it is based on grant table sharing and event channels mechanisms of Xen hypervisor. Grant table operations are used for transferring several KB of data consisting of multiple blocks. It provides an abstraction of a block device similar to SATA or SCSI with general read and write block device operations. It also supports command-reordering which means that commands might complete in an order different from the order in which they are issued.
\\
\\
As grant table and event channels are already ported to PHIDIAS, the only thing which needs to be done for setting up Xen's block virtual device is writing keys to Xenstore which are read by block frontend driver while finalizing the connection with backend. The frontend driver reads domain's device/vbd/0/backend key in the XenStore to get the location of back end for the virtual block device. The most important information it needs to read from Xenstore is related to sector size and number of sectors which is provided in listing \ref{xenblock}.

\begin{lstlisting}[caption= Keys in Xenstore for connecting virtual block device frontend driver with backend in Xen ,label={xenblock}]

/local/domain/0/backend/vbd/1/0/frontend-id 1
/local/domain/0/backend/vbd/1/0/frontend "/local/domain/1/device/vbd/0"
/local/domain/0/backend/vbd/1/0/sector-size
/local/domain/0/backend/vbd/1/0/size
/local/domain/0/backend/vbd/1/0/info

/local/domain/1/device/vbd/0/backend-id 0
/local/domain/1/device/vbd/0/backend "/local/domain/0/backend/vbd/1/0"

/local/domain/1/device/vbd/0/state 1
/local/domain/0/backend/vbd/1/0/state 1

\end{lstlisting}

Block frontend driver allocates a page for ring buffer for granting access to block backend driver. It writes ring−ref of shared ring page to Xenstore. It also allocates an unbounded event channel and passes it to backend through Xenstore. It reads Xenbus state of block backend and sets it own state while establishing connection. States are changed at different steps while talking with the other end until XenbusStateConnected is reached and set.
\\
\\
In PHIDIAS setup, since both guests are started at the same time unlike Dom0 and DomU in Xen, block frontend driver should be probed through an IPI triggered by Dom0 after successful running of Xenstore daemon and block blockend. The same approach used for netfront driver, as described in section \ref{sec:probenetfront}, could be followed.

\subsection{Porting miscellaneous Xen I/O Device Drivers \label{sec:misc}}
In this thesis, only network virtualization drivers of Xen had been ported to PHIDIAS. For remaining I/O devices e.g, keyboard, mouse, scsi, block devices etc, the necessary Xenstore keys read by frontend drivers should be written manually to Xenstore filesystem by Dom0 through a customized script. User could locate the necessary keys needed by frontend drivers by looking through their source code. For probing the frontend drivers, an IPC capability is needed to be configured in PHIDIAS and gets triggered by a simple kernel module from Dom0 to DomU.
